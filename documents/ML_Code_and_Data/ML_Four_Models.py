{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "606b68ea-d622-489a-af27-d4dd08132d68",
   "metadata": {},
   "source": [
    "# ML Classifers\n",
    "- KNN\n",
    "- Random Forest\n",
    "- SVM\n",
    "- AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cd0f4f-8c36-482e-aca7-abdb81d51c6b",
   "metadata": {},
   "source": [
    "## Classifer: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27a44f9a-587f-456d-a866-a1e095bee780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GENDER_1</th>\n",
       "      <th>GENDER_2</th>\n",
       "      <th>RACE_1</th>\n",
       "      <th>RACE_2</th>\n",
       "      <th>RACE_3</th>\n",
       "      <th>RACE_4</th>\n",
       "      <th>RACE_5</th>\n",
       "      <th>RACE_6</th>\n",
       "      <th>RACE_7</th>\n",
       "      <th>RACE_8</th>\n",
       "      <th>...</th>\n",
       "      <th>B2SENSTI_1.0</th>\n",
       "      <th>B2SENSTI_2.0</th>\n",
       "      <th>B2SENSTI_3.0</th>\n",
       "      <th>B2SENSTI_4.0</th>\n",
       "      <th>B2SENSTI_5.0</th>\n",
       "      <th>P7ATTENI</th>\n",
       "      <th>P7BEHAVE</th>\n",
       "      <th>P7SOLVE</th>\n",
       "      <th>P6SAMEAG</th>\n",
       "      <th>P4SAMEAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   GENDER_1  GENDER_2  RACE_1  RACE_2  RACE_3  RACE_4  RACE_5  RACE_6  RACE_7  \\\n",
       "0         0         1       1       0       0       0       0       0       0   \n",
       "1         0         1       1       0       0       0       0       0       0   \n",
       "\n",
       "   RACE_8  ...  B2SENSTI_1.0  B2SENSTI_2.0  B2SENSTI_3.0  B2SENSTI_4.0  \\\n",
       "0       0  ...             0             0             1             0   \n",
       "1       0  ...             0             0             1             0   \n",
       "\n",
       "   B2SENSTI_5.0  P7ATTENI  P7BEHAVE  P7SOLVE  P6SAMEAG  P4SAMEAG  \n",
       "0             0       1.0       1.0      1.0       1.0       0.0  \n",
       "1             0       1.0       1.0      0.0       1.0       1.0  \n",
       "\n",
       "[2 rows x 108 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pickle\n",
    "\n",
    "#read final dataset .csv file\n",
    "full_feature_df = pd.read_csv(\"preprocess_data.csv\")\n",
    "full_feature_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adef07bd-a2f5-467a-8766-ad1735aa6654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P2HITFUN_1.0</th>\n",
       "      <th>P2HITFUN_2.0</th>\n",
       "      <th>P2WARMCL_4.0</th>\n",
       "      <th>P2EXPRES_3.0</th>\n",
       "      <th>P2EXPRES_2.0</th>\n",
       "      <th>P2CHLIKE_2.0</th>\n",
       "      <th>P2CHLIKE_1.0</th>\n",
       "      <th>P2EXPRES_1.0</th>\n",
       "      <th>P2WARMCL_2.0</th>\n",
       "      <th>P2WARMCL_1.0</th>\n",
       "      <th>P2HITIGN_1.0</th>\n",
       "      <th>P2HITIGN_2.0</th>\n",
       "      <th>P4OPINIO_1.0</th>\n",
       "      <th>P2CHLIKE_3.0</th>\n",
       "      <th>P2WARMCL_3.0</th>\n",
       "      <th>P2HITCHO_1.0</th>\n",
       "      <th>P2HITCHO_2.0</th>\n",
       "      <th>P2CHLIKE_4.0</th>\n",
       "      <th>P4LISTEN_2.0</th>\n",
       "      <th>P2FEELAN_1.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   P2HITFUN_1.0  P2HITFUN_2.0  P2WARMCL_4.0  P2EXPRES_3.0  P2EXPRES_2.0  \\\n",
       "0             0             1             0             0             0   \n",
       "1             0             1             0             0             0   \n",
       "\n",
       "   P2CHLIKE_2.0  P2CHLIKE_1.0  P2EXPRES_1.0  P2WARMCL_2.0  P2WARMCL_1.0  \\\n",
       "0             0             1             1             0             1   \n",
       "1             0             1             1             0             1   \n",
       "\n",
       "   P2HITIGN_1.0  P2HITIGN_2.0  P4OPINIO_1.0  P2CHLIKE_3.0  P2WARMCL_3.0  \\\n",
       "0             0             1             0             0             0   \n",
       "1             0             1             0             0             0   \n",
       "\n",
       "   P2HITCHO_1.0  P2HITCHO_2.0  P2CHLIKE_4.0  P4LISTEN_2.0  P2FEELAN_1.0  \n",
       "0             0             1             0             0             0  \n",
       "1             0             1             0             0             0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce_feature_df = pd.read_csv(\"reduced_FS.csv\")\n",
    "reduce_feature_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f964d64-3744-4b10-97f5-a17d9a28e2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting the target labels from descriptive features\n",
    "X = full_feature_df.drop(columns=full_feature_df.columns[-5:]) #removes the target labels\n",
    "targets = full_feature_df.iloc[:, -5:] #get the target labels \n",
    "#fix the ranking to 1-4\n",
    "for label in targets.columns:\n",
    "    unique_values = targets[label].unique()\n",
    "\n",
    "    # Create a mapping to increment each unique value by 1\n",
    "    value_mapping = {val: val + 1 for val in unique_values}\n",
    "    #print(\"Value mapping:\", value_mapping)\n",
    "\n",
    "    # Apply the mapping to the 'target' column\n",
    "    targets[label] = targets[label].map(value_mapping)\n",
    "\n",
    "for column in targets.columns:\n",
    "    print(f\"Class distribution for {column}:\")\n",
    "    print(targets[column].value_counts())\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bfe2f3-a2dd-4021-ad3c-f5c6db335cad",
   "metadata": {},
   "source": [
    "# KNN (full features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c592fef-2a30-4dea-bc24-fe88d96da65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for finalized_model_M1.sav (P7ATTENI): 0.8428167459824816\n",
      "Score for finalized_model_M2.sav (P7BEHAVE): 0.8614681330329932\n",
      "Score for finalized_model_M3.sav (P7SOLVE): 0.8280063064354307\n",
      "Score for finalized_model_M4.sav (P6SAMEAG): 0.8391160380074864\n",
      "Score for finalized_model_M5.sav (P4SAMEAG): 0.8233003660151078\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# File names for KNN full feature set\n",
    "file_names = ['finalized_model_M1.sav', 'finalized_model_M2.sav', \n",
    "              'finalized_model_M3.sav', 'finalized_model_M4.sav', \n",
    "              'finalized_model_M5.sav']\n",
    "\n",
    "# Ensure target labels are integers\n",
    "targets = targets.astype(int)\n",
    "\n",
    "# Index for file_names\n",
    "index = 0\n",
    "# Iterate over each target label\n",
    "for index, target_label in enumerate(targets.columns):\n",
    "    if index >= len(file_names):\n",
    "        print(\"Not enough filenames provided for all target labels.\")\n",
    "        break\n",
    "\n",
    "    y = targets[target_label]  # Select specific target label\n",
    "\n",
    "    # Check class diversity in the target label\n",
    "    class_counts = y.value_counts()\n",
    "    if len(class_counts) <= 1:\n",
    "        print(f\"Skipping {target_label} due to lack of class diversity: {class_counts}\")\n",
    "        continue\n",
    "\n",
    "    # Balance the dataset using SMOTE\n",
    "    try:\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during SMOTE for {target_label}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Split the balanced data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_balanced, y_balanced, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Create and train the KNN model\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Save the model to disk\n",
    "    filename = file_names[index]\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(knn, file)\n",
    "\n",
    "    # Load the model from disk (optional)\n",
    "    loaded_model = pickle.load(open(filename, 'rb'))\n",
    "    result = loaded_model.score(X_test, y_test)\n",
    "\n",
    "    # Print the score for this target label\n",
    "    print(f\"Score for {filename} ({target_label}): {result}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fffd5b7-41e1-469a-8b26-a11a4ddbc380",
   "metadata": {},
   "source": [
    "# KNN (reduced set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb320ba2-230f-446e-aa4e-dab803f11100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File names for KNN reduced feature set\n",
    "file_names = ['finalized_model_M6.sav', 'finalized_model_M7.sav', \n",
    "              'finalized_model_M8.sav', 'finalized_model_M9.sav', \n",
    "              'finalized_model_M10.sav']\n",
    "\n",
    "#get reduced features \n",
    "X = reduce_feature_df\n",
    "\n",
    "# Ensure target labels are integers\n",
    "targets = targets.astype(int)\n",
    "\n",
    "# Index for file_names\n",
    "index = 0\n",
    "\n",
    "#get the target labels\n",
    "targets = full_feature_df.iloc[:, -5:]  \n",
    "\n",
    "# Iterate over each target label\n",
    "for index, target_label in enumerate(targets.columns):\n",
    "    if index >= len(file_names):\n",
    "        print(\"Not enough filenames provided for all target labels.\")\n",
    "        break\n",
    "\n",
    "    y = targets[target_label]  # Select specific target label\n",
    "\n",
    "    # Check class diversity in the target label\n",
    "    class_counts = y.value_counts()\n",
    "    if len(class_counts) <= 1:\n",
    "        print(f\"Skipping {target_label} due to lack of class diversity: {class_counts}\")\n",
    "        continue\n",
    "        \n",
    "    # Balance the dataset using SMOTE\n",
    "    try:\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during SMOTE for {target_label}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Split the balanced data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_balanced, y_balanced, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(f\"the TEST size of 20% is {len(y_test)}\\n\")\n",
    "\n",
    "    # Create and train the KNN model\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Save the model to disk\n",
    "    filename = file_names[index]\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(knn, file)\n",
    "\n",
    "    # Load the model from disk (optional)\n",
    "    loaded_model = pickle.load(open(filename, 'rb'))\n",
    "    result = loaded_model.score(X_test, y_test)\n",
    "\n",
    "    # Print the score for this target label\n",
    "    print(f\"Score for {filename} ({target_label}): {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2186c3db-51e8-407f-baf7-f35e71d8bffd",
   "metadata": {},
   "source": [
    "# Random Forest (full features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cad26f46-d0c7-493d-9754-e07b7b7bcdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for finalized_model_M11.sav (P7ATTENI): 0.9013725084488585\n",
      "\n",
      "Score for finalized_model_M12.sav (P7BEHAVE): 0.9250695640651915\n",
      "\n",
      "Score for finalized_model_M13.sav (P7SOLVE): 0.893077253834026\n",
      "\n",
      "Score for finalized_model_M14.sav (P6SAMEAG): 0.8962712352433055\n",
      "\n",
      "Score for finalized_model_M15.sav (P4SAMEAG): 0.863406276769722\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# File names for the Random Forest full features\n",
    "file_names = ['finalized_model_M11.sav', 'finalized_model_M12.sav', 'finalized_model_M13.sav', 'finalized_model_M14.sav', \n",
    "              'finalized_model_M15.sav']\n",
    "\n",
    "#get full features \n",
    "X = full_feature_df.drop(columns=full_feature_df.columns[-5:]) #removes the target labels\n",
    "\n",
    "#get the target labels\n",
    "targets = full_feature_df.iloc[:, -5:]  \n",
    "\n",
    "for label in targets.columns:\n",
    "    unique_values = sorted(targets[label].unique())  # Get sorted unique values\n",
    "    value_mapping = {val: rank for rank, val in enumerate(unique_values, start=1)}  # Create rank mapping (1–4)\n",
    "    targets[label] = targets[label].map(value_mapping)  # Apply the rank mapping\n",
    "\n",
    "# Index for file_names\n",
    "index = 0  \n",
    "\n",
    "# Ensure target labels are integers\n",
    "targets = targets.astype(int)\n",
    "\n",
    "# Iterate over each target label\n",
    "for index, target_label in enumerate(targets.columns):\n",
    "    if index >= len(file_names):\n",
    "        print(\"Not enough filenames provided for all target labels.\")\n",
    "        break\n",
    "        \n",
    "    y = targets[target_label]  # Extract target values for the current column\n",
    "\n",
    "    # Check class diversity in the target label\n",
    "    class_counts = y.value_counts()\n",
    "    if len(class_counts) <= 1:\n",
    "        print(f\"Skipping {target_label} due to lack of class diversity: {class_counts}\")\n",
    "        continue\n",
    "\n",
    "    # Balance the dataset using SMOTE\n",
    "    try:\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during SMOTE for {target_label}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Split the balanced data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)\n",
    "    \n",
    "\n",
    "    # Create Random Forest classifier instance\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust n_estimators and other parameters\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    #Save the model to disk using pickle\n",
    "    filename = file_names[index]\n",
    "    pickle.dump(rf, open(filename, 'wb'))\n",
    "    \n",
    "    # Save the model to disk\n",
    "    filename = file_names[index]\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(rf, file)\n",
    "\n",
    "    # Load the model from disk (optional)\n",
    "    loaded_model = pickle.load(open(filename, 'rb'))\n",
    "    result = loaded_model.score(X_test, y_test)\n",
    "\n",
    "    # Print the score for this target label\n",
    "    print(f\"Score for {filename} ({target_label}): {result}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0434f9-fa83-4d07-a4de-df13de3bcc35",
   "metadata": {},
   "source": [
    "# Random Forest (reduced features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9839ade4-3e30-4138-9d47-d7341ac819f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the TEST size of 20% is 14499\n",
      "\n",
      "Score for finalized_model_M16.sav (P7ATTENI): 0.3026415614869991\n",
      "\n",
      "the TEST size of 20% is 15094\n",
      "\n",
      "Score for finalized_model_M17.sav (P7BEHAVE): 0.32940241155426\n",
      "\n",
      "the TEST size of 20% is 13954\n",
      "\n",
      "Score for finalized_model_M18.sav (P7SOLVE): 0.31711337250967464\n",
      "\n",
      "the TEST size of 20% is 13892\n",
      "\n",
      "Score for finalized_model_M19.sav (P6SAMEAG): 0.3152893751799597\n",
      "\n",
      "the TEST size of 20% is 12841\n",
      "\n",
      "Score for finalized_model_M20.sav (P4SAMEAG): 0.3182773927264232\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# File names for the Random Forest reduced features\n",
    "file_names = ['finalized_model_M16.sav', 'finalized_model_M17.sav', 'finalized_model_M18.sav', 'finalized_model_M19.sav', \n",
    "              'finalized_model_M20.sav']\n",
    "\n",
    "#get reduced features \n",
    "X = reduce_feature_df\n",
    "\n",
    "#get the target labels\n",
    "targets = full_feature_df.iloc[:, -5:]  \n",
    "\n",
    "for label in targets.columns:\n",
    "    unique_values = sorted(targets[label].unique())  # Get sorted unique values\n",
    "    value_mapping = {val: rank for rank, val in enumerate(unique_values, start=1)}  # Create rank mapping (1–4)\n",
    "    targets[label] = targets[label].map(value_mapping)  # Apply the rank mapping\n",
    "# Index for file_names\n",
    "index = 0  \n",
    "\n",
    "# Ensure target labels are integers\n",
    "targets = targets.astype(int)\n",
    "\n",
    "# Iterate over each target label\n",
    "for index, target_label in enumerate(targets.columns):\n",
    "    if index >= len(file_names):\n",
    "        print(\"Not enough filenames provided for all target labels.\")\n",
    "        break\n",
    "    # Extract target values for the current column\n",
    "    y = targets[target_label]  \n",
    "\n",
    "    # Check class diversity in the target label\n",
    "    class_counts = y.value_counts()\n",
    "    if len(class_counts) <= 1:\n",
    "        print(f\"Skipping {target_label} due to lack of class diversity: {class_counts}\")\n",
    "        continue\n",
    "\n",
    "    # Balance the dataset using SMOTE\n",
    "    try:\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during SMOTE for {target_label}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Split the balanced data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(f\"the TEST size of 20% is {len(y_test)}\\n\")\n",
    "\n",
    "    # Create Random Forest classifier instance\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)  \n",
    "    \n",
    "    # Train the model on the training data\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    #Save the model to disk using pickle\n",
    "    filename = file_names[index]\n",
    "    pickle.dump(rf, open(filename, 'wb'))\n",
    "    \n",
    "    # Save the model to disk\n",
    "    filename = file_names[index]\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(rf, file)\n",
    "\n",
    "    # Load the model from disk (optional)\n",
    "    loaded_model = pickle.load(open(filename, 'rb'))\n",
    "    result = loaded_model.score(X_test, y_test)\n",
    "\n",
    "    # Print the score for this target label\n",
    "    print(f\"Score for {filename} ({target_label}): {result}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9079c074-57a0-4a2a-80c2-f1bfe84ae1ef",
   "metadata": {},
   "source": [
    "# Simple Vector Machine (Full Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55f6ce88-c5b4-415b-84d0-20b4089b504d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for finalized_model_M21.sav (P7ATTENI): 0.7859162700875922\n",
      "\n",
      "Score for finalized_model_M22.sav (P7BEHAVE): 0.8525241817940904\n",
      "\n",
      "Score for finalized_model_M23.sav (P7SOLVE): 0.8060771105059481\n",
      "\n",
      "Score for finalized_model_M24.sav (P6SAMEAG): 0.8296861503023323\n",
      "\n",
      "Score for finalized_model_M25.sav (P4SAMEAG): 0.8026633439763259\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "# File names for the SVM full features\n",
    "file_names = ['finalized_model_M21.sav', 'finalized_model_M22.sav', 'finalized_model_M23.sav', 'finalized_model_M24.sav', \n",
    "              'finalized_model_M25.sav']\n",
    "\n",
    "#removes the target labels\n",
    "X = full_feature_df.drop(columns=full_feature_df.columns[-5:]) \n",
    "\n",
    "# Ensure target labels are integers\n",
    "targets = full_feature_df.iloc[:, -5:].astype(int)  \n",
    "\n",
    "for label in targets.columns:\n",
    "    unique_values = sorted(targets[label].unique())  # Get sorted unique values\n",
    "    value_mapping = {val: rank for rank, val in enumerate(unique_values, start=1)}  # Create rank mapping (1–4)\n",
    "    targets[label] = targets[label].map(value_mapping)  # Apply the rank mapping\n",
    "#filename index\n",
    "index = 0\n",
    "\n",
    "# Iterate over each target label\n",
    "for index, target_label in enumerate(targets.columns):\n",
    "    if index >= len(file_names):\n",
    "        print(\"Not enough filenames provided for all target labels.\")\n",
    "        break\n",
    "    # Extract target values for the current column    \n",
    "    y = targets[target_label]  \n",
    "    \n",
    "    # Check class diversity in the target label\n",
    "    class_counts = y.value_counts()\n",
    "    if len(class_counts) <= 1:\n",
    "        print(f\"Skipping {target_label} due to lack of class diversity: {class_counts}\")\n",
    "        continue\n",
    "\n",
    "    # Balance the dataset using SMOTE\n",
    "    try:\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during SMOTE for {target_label}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Split the balanced data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create SVM classifier instance\n",
    "    svm = SVC(kernel='rbf', random_state=42) \n",
    "    \n",
    "    # Train the model on the training data\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    # Save the model to disk using pickle\n",
    "    filename = file_names[index]\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(svm, file)\n",
    "\n",
    "    # Load the model from disk (optional)\n",
    "    loaded_model = pickle.load(open(filename, 'rb'))\n",
    "    result = loaded_model.score(X_test, y_test)\n",
    "\n",
    "    # Print the score for this target label\n",
    "    print(f\"Score for {filename} ({target_label}): {result}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bea2f5d-9011-42c1-810e-3f264c4568de",
   "metadata": {},
   "source": [
    "# SVM (Reduced Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4654dfcc-a15e-42eb-8f2a-019cfe73a15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TEST size of 20% is 14499\n",
      "\n",
      "Score for finalized_model_M26.sav (P7ATTENI): 0.29326160424856884\n",
      "\n",
      "The TEST size of 20% is 15094\n",
      "\n",
      "Score for finalized_model_M27.sav (P7BEHAVE): 0.3136345567775275\n",
      "\n",
      "The TEST size of 20% is 13954\n",
      "\n",
      "Score for finalized_model_M28.sav (P7SOLVE): 0.29704744159380825\n",
      "\n",
      "The TEST size of 20% is 13892\n",
      "\n",
      "Score for finalized_model_M29.sav (P6SAMEAG): 0.3013964871868701\n",
      "\n",
      "The TEST size of 20% is 12841\n",
      "\n",
      "Score for finalized_model_M30.sav (P4SAMEAG): 0.2956156062611946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "# File names for the SVM reduced features\n",
    "file_names = ['finalized_model_M26.sav', 'finalized_model_M27.sav', 'finalized_model_M28.sav', 'finalized_model_M29.sav', \n",
    "              'finalized_model_M30.sav']\n",
    "#removes the target labels\n",
    "X = reduce_feature_df \n",
    "\n",
    "# Ensure target labels are integers\n",
    "targets = full_feature_df.iloc[:, -5:].astype(int)  # Adjusted if needed\n",
    "\n",
    "for label in targets.columns:\n",
    "    unique_values = sorted(targets[label].unique())  # Get sorted unique values\n",
    "    value_mapping = {val: rank for rank, val in enumerate(unique_values, start=1)}  # Create rank mapping (1–4)\n",
    "    targets[label] = targets[label].map(value_mapping)  # Apply the rank mapping\n",
    "#filename index\n",
    "index = 0\n",
    "\n",
    "# Iterate over each target label\n",
    "for index, target_label in enumerate(targets.columns):\n",
    "    if index >= len(file_names):\n",
    "        print(\"Not enough filenames provided for all target labels.\")\n",
    "        break\n",
    "    # Extract target values for the current column    \n",
    "    y = targets[target_label]  \n",
    "    \n",
    "    # Check class diversity in the target label\n",
    "    class_counts = y.value_counts()\n",
    "    if len(class_counts) <= 1:\n",
    "        print(f\"Skipping {target_label} due to lack of class diversity: {class_counts}\")\n",
    "        continue\n",
    "\n",
    "    # Balance the dataset using SMOTE\n",
    "    try:\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during SMOTE for {target_label}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Split the balanced data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create SVM classifier instance\n",
    "    svm = SVC(kernel='rbf', random_state=42)\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    # Save the model to disk using pickle\n",
    "    filename = file_names[index]\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(svm, file)\n",
    "\n",
    "    # Load the model from disk (optional)\n",
    "    loaded_model = pickle.load(open(filename, 'rb'))\n",
    "    result = loaded_model.score(X_test, y_test)\n",
    "\n",
    "    # Print the score for this target label\n",
    "    print(f\"Score for {filename} ({target_label}): {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d7aa9a-1091-4ae6-a1b9-4b04e664dd97",
   "metadata": {},
   "source": [
    "# ADABoost (Full Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3857925b-184e-4fff-bbbb-a03178234a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for finalized_model_M31.sav (P7ATTENI): 0.8575842851625007\n",
      "\n",
      "Accuracy: 0.86\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.79      0.79      4531\n",
      "           2       0.78      0.79      0.79      4531\n",
      "           3       0.87      0.87      0.87      4530\n",
      "           4       0.98      0.98      0.98      4531\n",
      "\n",
      "    accuracy                           0.86     18123\n",
      "   macro avg       0.86      0.86      0.86     18123\n",
      "weighted avg       0.86      0.86      0.86     18123\n",
      "\n",
      "------------------------------\n",
      "Score for finalized_model_M32.sav (P7BEHAVE): 0.9016801823289341\n",
      "\n",
      "Accuracy: 0.90\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.81      0.83      4717\n",
      "           2       0.81      0.85      0.83      4717\n",
      "           3       0.95      0.95      0.95      4716\n",
      "           4       0.99      0.99      0.99      4717\n",
      "\n",
      "    accuracy                           0.90     18867\n",
      "   macro avg       0.90      0.90      0.90     18867\n",
      "weighted avg       0.90      0.90      0.90     18867\n",
      "\n",
      "------------------------------\n",
      "Score for finalized_model_M33.sav (P7SOLVE): 0.8461185643848183\n",
      "\n",
      "Accuracy: 0.85\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.73      0.74      4361\n",
      "           2       0.75      0.76      0.76      4361\n",
      "           3       0.90      0.91      0.90      4360\n",
      "           4       0.98      0.99      0.99      4360\n",
      "\n",
      "    accuracy                           0.85     17442\n",
      "   macro avg       0.85      0.85      0.85     17442\n",
      "weighted avg       0.85      0.85      0.85     17442\n",
      "\n",
      "------------------------------\n",
      "Score for finalized_model_M34.sav (P6SAMEAG): 0.8667434494673193\n",
      "\n",
      "Accuracy: 0.87\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.77      0.74      0.76      4342\n",
      "           2       0.75      0.79      0.77      4341\n",
      "           3       0.95      0.94      0.94      4341\n",
      "           4       0.99      0.99      0.99      4341\n",
      "\n",
      "    accuracy                           0.87     17365\n",
      "   macro avg       0.87      0.87      0.87     17365\n",
      "weighted avg       0.87      0.87      0.87     17365\n",
      "\n",
      "------------------------------\n",
      "Score for finalized_model_M35.sav (P4SAMEAG): 0.8254314372936266\n",
      "\n",
      "Accuracy: 0.83\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.70      0.69      0.69      4013\n",
      "           2       0.70      0.71      0.70      4013\n",
      "           3       0.91      0.92      0.91      4012\n",
      "           4       0.99      0.99      0.99      4013\n",
      "\n",
      "    accuracy                           0.83     16051\n",
      "   macro avg       0.82      0.83      0.83     16051\n",
      "weighted avg       0.82      0.83      0.83     16051\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "\n",
    "# File names for the AdaBoost full features\n",
    "file_names = ['finalized_model_M31.sav', 'finalized_model_M32.sav', 'finalized_model_M33.sav', 'finalized_model_M34.sav', \n",
    "              'finalized_model_M35.sav']\n",
    "\n",
    "\n",
    "# Prepare the dataset (assuming full_feature_df is a pandas DataFrame)\n",
    " # Removes the target labels\n",
    "X = full_feature_df.drop(columns=full_feature_df.columns[-5:]) \n",
    " # Extract the target labels\n",
    "targets = full_feature_df.iloc[:, -5:].astype(int) \n",
    "\n",
    "for label in targets.columns:\n",
    "    unique_values = sorted(targets[label].unique())  # Get sorted unique values\n",
    "    value_mapping = {val: rank for rank, val in enumerate(unique_values, start=1)}  # Create rank mapping (1–4)\n",
    "    targets[label] = targets[label].map(value_mapping)  # Apply the rank mapping\n",
    "    \n",
    "#le = LabelEncoder()\n",
    "\n",
    "#filename index\n",
    "index = 0\n",
    "# Iterate through each target label\n",
    "for index, target_label in enumerate(targets.columns):\n",
    "    # Ensure there are enough filenames if applicable\n",
    "    if index >= len(file_names):\n",
    "        print(\"Not enough filenames provided for all target labels.\")\n",
    "        break\n",
    "\n",
    "     # Extract target values for the current column\n",
    "    y = targets[target_label] \n",
    "    #y = le.fit_transform(y)\n",
    "\n",
    "    # Check class diversity in the target label\n",
    "    class_counts = pd.Series(y).value_counts()\n",
    "    if len(class_counts) <= 1:\n",
    "        print(f\"Skipping {target_label} due to lack of class diversity: {class_counts}\")\n",
    "        continue\n",
    "\n",
    "    # Balance the dataset using SMOTE\n",
    "    try:\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during SMOTE for {target_label}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_balanced, y_balanced, stratify=y_balanced, random_state=0\n",
    "    )\n",
    "\n",
    "    # Train a Decision Tree as the base classifier for AdaBoost\n",
    "    base_tree = DecisionTreeClassifier(max_depth=20, random_state=42)\n",
    "    adaboost = AdaBoostClassifier(\n",
    "        estimator=base_tree,\n",
    "        n_estimators=50,\n",
    "        learning_rate=1.0,\n",
    "        algorithm='SAMME',  \n",
    "        random_state=42\n",
    "    )\n",
    "    adaboost.fit(X_train, y_train)\n",
    "\n",
    "    filename = file_names[index]\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(adaboost, file)\n",
    "\n",
    "    loaded_model = pickle.load(open(filename, 'rb'))\n",
    "    result = loaded_model.score(X_test, y_test)\n",
    "\n",
    "    # Print the score for this target label\n",
    "    print(f\"Score for {filename} ({target_label}): {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641ab46e-ce2e-465f-961b-66ca94d78522",
   "metadata": {},
   "source": [
    "# AdaBoost (Reduced Features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b72cb138-d5ef-4206-9c12-35415254432a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for finalized_model_M36.sav (P7ATTENI): 0.275671798267395\n",
      "\n",
      "Score for finalized_model_M37.sav (P7BEHAVE): 0.26501298563629616\n",
      "\n",
      "Score for finalized_model_M38.sav (P7SOLVE): 0.2624125673661277\n",
      "\n",
      "Score for finalized_model_M39.sav (P6SAMEAG): 0.26651310106536136\n",
      "\n",
      "Score for finalized_model_M40.sav (P4SAMEAG): 0.2589869790044234\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# File names for the AdaBoost reduced features \n",
    "file_names = ['finalized_model_M36.sav', 'finalized_model_M37.sav', 'finalized_model_M38.sav', 'finalized_model_M39.sav', \n",
    "              'finalized_model_M40.sav']\n",
    "\n",
    "# Prepare the dataset (assuming reduce_feature_df and full_feature_df are pandas DataFrames)\n",
    "X = reduce_feature_df  # Reduced feature set\n",
    "targets = full_feature_df.iloc[:, -5:].astype(int)  # Extract the target labels\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "#filename index\n",
    "index = 0\n",
    "\n",
    "# Iterate through each target label\n",
    "for index, target_label in enumerate(targets.columns):\n",
    "    # Ensure there are enough filenames if applicable\n",
    "    if index >= len(file_names):\n",
    "        print(\"Not enough filenames provided for all target labels.\")\n",
    "        break\n",
    "\n",
    "    y = targets[target_label]  # Extract target values for the current column\n",
    "    y = le.fit_transform(y)\n",
    "\n",
    "    # Check class diversity in the target label\n",
    "    class_counts = pd.Series(y).value_counts()\n",
    "    if len(class_counts) <= 1:\n",
    "        print(f\"Skipping {target_label} due to lack of class diversity: {class_counts}\")\n",
    "        continue\n",
    "\n",
    "    # Balance the dataset using SMOTE\n",
    "    try:\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during SMOTE for {target_label}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_balanced, y_balanced, stratify=y_balanced, random_state=0\n",
    "    )\n",
    "\n",
    "    # Train a Decision Tree as the base classifier for AdaBoost\n",
    "    base_tree = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "    adaboost = AdaBoostClassifier(\n",
    "        estimator=base_tree,\n",
    "        n_estimators=50,\n",
    "        learning_rate=1.0,\n",
    "        algorithm='SAMME',\n",
    "        random_state=42\n",
    "    )\n",
    "    adaboost.fit(X_train, y_train)\n",
    "\n",
    "    # Save the AdaBoost model to a file\n",
    "    filename = file_names[index]\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(adaboost, file)\n",
    "\n",
    "    loaded_model = pickle.load(open(filename, 'rb'))\n",
    "    result = loaded_model.score(X_test, y_test)\n",
    "\n",
    "    # Print the score for this target label\n",
    "    print(f\"Score for {filename} ({target_label}): {result}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
